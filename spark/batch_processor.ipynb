{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, functions as F, types as T, Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Spark configuration and context\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Set up the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchProcessor\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Xss4m\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xss4m\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"authors\", T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"date_google\", T.StringType(), True),\n",
    "    T.StructField(\"date_metadata\", T.StringType(), True),\n",
    "    T.StructField(\"date_published\", T.StringType(), True),\n",
    "    T.StructField(\"date_target\", T.StringType(), True),\n",
    "    T.StructField(\"description\", T.StringType(), True),\n",
    "    T.StructField(\"explanation\", T.StringType(), True),\n",
    "    T.StructField(\"groq_usage\", T.StringType(), True),\n",
    "    T.StructField(\"metadata\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "    T.StructField(\"rating_democrats\", T.FloatType(), True),\n",
    "    T.StructField(\"rating_republicans\", T.FloatType(), True),\n",
    "    T.StructField(\"source_url\", T.StringType(), True),\n",
    "    T.StructField(\"summary\", T.StringType(), True),\n",
    "    T.StructField(\"text\", T.StringType(), True),\n",
    "    T.StructField(\"title\", T.StringType(), True),\n",
    "    T.StructField(\"url\", T.StringType(), True)\n",
    "])\n",
    "news_df = spark.read.option(\"multiline\", \"true\").json(\"../news_ratings/data/\", schema=schema)\n",
    "news_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df = spark.read.csv(\"../stocks_data/ticker_data.csv\", header=True, inferSchema=True)\n",
    "market_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.withColumn(\"published_at\", F.coalesce(\"date_google\", \"date_metadata\", \"date_published\")) \\\n",
    "    .withColumn(\"published_at\", F.to_timestamp(\"published_at\")) \\\n",
    "    .withColumn(\"date_target\", F.to_date(\"date_target\"))\n",
    "news_df = news_df.select('published_at', 'date_target', 'rating_democrats', 'rating_republicans', 'title', 'summary', 'url')\n",
    "news_df = news_df.na.drop(subset=[\"published_at\"])\n",
    "news_df.sample(fraction=0.01).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df = market_df.filter(market_df['Ticker'] == \"JPM\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_windows = [\n",
    "#   1,      # 1 hour\n",
    "#   5,      # 5 hours\n",
    "  10,     # 10 hours\n",
    "  24,     # 1 day\n",
    "  7*24,   # 1 week\n",
    "#   14*24,  # 2 weeks\n",
    "#   28*24,  # 4 weeks\n",
    "]\n",
    "# --------------------\n",
    "statistics = [\n",
    "  \"count\",\n",
    "  \"mean\",\n",
    "  \"std\",\n",
    "  \"min\",\n",
    "  \"max\",\n",
    "  \"median\",\n",
    "  \"spread\",\n",
    "]\n",
    "# --------------------\n",
    "ticker_cols = [\n",
    "  \"Open\",\n",
    "  \"High\",\n",
    "  \"Low\",\n",
    "  \"Close\",\n",
    "  \"Adj Close\",\n",
    "  \"Volume\"\n",
    "]\n",
    "# --------------------\n",
    "news_cols = [\n",
    "  \"rating_republicans\",\n",
    "  \"rating_democrats\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for rolling window calculations\n",
    "def calculate_rolling_stats(df, cols, datetime_col, partition_col=None):\n",
    "    \"\"\"\n",
    "    Computes rolling statistics for a given column over various time windows.\n",
    "    \"\"\"\n",
    "    result_df = df\n",
    "    for window_hours in time_windows:\n",
    "        # Define the window range in seconds\n",
    "        window_range = window_hours * 3600\n",
    "        # Define a rolling window spec\n",
    "        window_spec = (\n",
    "            W\n",
    "            .partitionBy(partition_col if partition_col else [])\n",
    "            .orderBy(F.col(datetime_col).cast(\"timestamp\").cast(\"long\"))\n",
    "            .rangeBetween(-window_range, 0)\n",
    "        )\n",
    "        for col in cols:\n",
    "            for stat in statistics:\n",
    "                col_name = f\"rolling_{window_hours}h_{col}_{stat}\"\n",
    "                if   stat == \"count\":   result_df = result_df.withColumn(col_name, F.count(col).over(window_spec))\n",
    "                elif stat == \"mean\":    result_df = result_df.withColumn(col_name, F.mean(col).over(window_spec))\n",
    "                elif stat == \"std\":     result_df = result_df.withColumn(col_name, F.stddev(col).over(window_spec))\n",
    "                elif stat == \"min\":     result_df = result_df.withColumn(col_name, F.min(col).over(window_spec))\n",
    "                elif stat == \"max\":     result_df = result_df.withColumn(col_name, F.max(col).over(window_spec))\n",
    "                elif stat == \"median\":  result_df = result_df.withColumn(col_name, F.approx_percentile(col, 0.5, 10).over(window_spec))\n",
    "                elif stat == \"spread\":  result_df = result_df.withColumn(col_name, F.max(col).over(window_spec) - F.min(col).over(window_spec))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "market_rollstats_df = calculate_rolling_stats(market_df, ticker_cols, \"Date\", \"Ticker\")\n",
    "news_rollstats_df = calculate_rolling_stats(news_df, news_cols, \"published_at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "sc.stop()       # Stop the Spark contex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
